{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osaeed-ds/vector-hello/blob/main/FieldLearning_DevFramework_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Field Learning - Langchain**\n"
      ],
      "metadata": {
        "id": "pvPcHxhErXbg"
      },
      "id": "pvPcHxhErXbg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLHkt-bMq4up"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "In this exercise we are going to utilize 3 modules from Langchain\n",
        "- Prompts (Prompt Templates)\n",
        "- Chains\n",
        "- Retrieval (Vector Store of Astra DB)\n",
        "\n"
      ],
      "id": "oLHkt-bMq4up"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQUeV_S5q4u0"
      },
      "source": [
        "## **Prerequisites Setup**\n"
      ],
      "id": "WQUeV_S5q4u0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1p8iUgjq4u2"
      },
      "source": [
        "* Follow [these steps](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html) to create a new vector search enabled database in Astra.\n",
        "* Generate a new [\"Database Administrator\" token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html)\n",
        "* Download the secure connect bundle for the database you just created (you can do this from the \"Connect\" tab of your database.\n",
        "* You will also need the necessary secret for the LLM provider of your choice:\n",
        " - If Open AI, then you will need an [Open AI API Key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key). This will require an Open AI account with billing enabled\n",
        " - If Vertex AI, you will need a config file\n",
        " - For more details, see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org.\n"
      ],
      "id": "Z1p8iUgjq4u2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2953d95b",
      "metadata": {
        "id": "2953d95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48ad1c45-07ad-4a98-8c0a-fa158f3ad70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.34.0-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.0.308-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cassio\n",
            "  Downloading cassio-0.1.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.10.0)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
            "  Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.0/321.0 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely<2.0.0 (from google-cloud-aiplatform)\n",
            "  Downloading Shapely-1.8.5.post1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.40 (from langchain)\n",
            "  Downloading langsmith-0.0.42-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting cassandra-driver>=3.28.0 (from cassio)\n",
            "  Downloading cassandra_driver-3.28.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver>=3.28.0->cassio) (1.16.0)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver>=3.28.0->cassio)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.58.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.6.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver>=3.28.0->cassio) (8.1.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Installing collected packages: xxhash, shapely, mypy-extensions, marshmallow, jsonpointer, geomet, dill, typing-inspect, tiktoken, multiprocess, langsmith, jsonpatch, huggingface-hub, cassandra-driver, openai, dataclasses-json, cassio, langchain, datasets, google-cloud-resource-manager, google-cloud-aiplatform\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.1\n",
            "    Uninstalling shapely-2.0.1:\n",
            "      Successfully uninstalled shapely-2.0.1\n",
            "Successfully installed cassandra-driver-3.28.0 cassio-0.1.3 dataclasses-json-0.6.1 datasets-2.14.5 dill-0.3.7 geomet-0.2.1.post1 google-cloud-aiplatform-1.34.0 google-cloud-resource-manager-1.10.4 huggingface-hub-0.17.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.308 langsmith-0.0.42 marshmallow-3.20.1 multiprocess-0.70.15 mypy-extensions-1.0.0 openai-0.28.1 shapely-1.8.5.post1 tiktoken-0.5.1 typing-inspect-0.9.0 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# install required dependencies\n",
        "! pip install datasets google-cloud-aiplatform openai pandas tiktoken langchain cassio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222f44ff",
      "metadata": {
        "id": "222f44ff"
      },
      "source": [
        "You may be asked to \"Restart the Runtime\" at this time, as some dependencies\n",
        "have been upgraded. **Please do restart the runtime now** for a smoother execution from this point onward."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Astra DB Setup**\n",
        "The following steps will ask for the keyspace of the vector search enabled Astra DB that you want to use for this example, as well as the Astra DB Token that you generated as part of the prerequisites. You will also require to upload the [Secure Connect Bundle](https://awesome-astra.github.io/docs/pages/astra/download-scb/#c-procedure).\n",
        "Lastly, we are going to create helper functions for a secure connection to Astra DB `getCQLSession` `getCQLKeyspace` and `getTableCount`"
      ],
      "metadata": {
        "id": "qSZ3-OG5Kdh6"
      },
      "id": "qSZ3-OG5Kdh6"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zh4P-XUDq4u9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff316a2a-0070-44c7-ff51-4a8727134530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Astra DB Keyspace name: vector_preview\n"
          ]
        }
      ],
      "source": [
        "# Input your database keyspace name:\n",
        "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
      ],
      "id": "zh4P-XUDq4u9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lThGqYchq4u-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b90a81-05e0-43c1-e87a-425bbb47ca97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your Astra DB Token: ··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
        "ASTRA_DB_TOKEN_BASED_PASSWORD = getpass('Your Astra DB Token: ')"
      ],
      "id": "lThGqYchq4u-"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xnNziXZ1q4vD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "1fca9ebb-3529-4e24-e0ce-c78c86c8cb04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Secure Connect Bundle\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5666917-fdce-4142-8d71-949dd691d98f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5666917-fdce-4142-8d71-949dd691d98f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving secure-connect-osaeed-vector.zip to secure-connect-osaeed-vector.zip\n"
          ]
        }
      ],
      "source": [
        "# Upload your Secure Connect Bundle zipfile:\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "print('Please upload your Secure Connect Bundle')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
        "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
        "else:\n",
        "    raise ValueError(\n",
        "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
        "    )"
      ],
      "id": "xnNziXZ1q4vD"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TUDw-07Iq4vE"
      },
      "outputs": [],
      "source": [
        "# colab-specific override of helper functions\n",
        "from cassandra.cluster import (\n",
        "    Cluster,\n",
        ")\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "# The \"username\" is the literal string 'token' for this connection mode:\n",
        "ASTRA_DB_TOKEN_BASED_USERNAME = 'token'\n",
        "\n",
        "\n",
        "def getCQLSession(mode='astra_db'):\n",
        "    if mode == 'astra_db':\n",
        "        cluster = Cluster(\n",
        "            cloud={\n",
        "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
        "            },\n",
        "            auth_provider=PlainTextAuthProvider(\n",
        "                ASTRA_DB_TOKEN_BASED_USERNAME,\n",
        "                ASTRA_DB_TOKEN_BASED_PASSWORD,\n",
        "            ),\n",
        "        )\n",
        "        astraSession = cluster.connect()\n",
        "        return astraSession\n",
        "    else:\n",
        "        raise ValueError('Unsupported CQL Session mode')\n",
        "\n",
        "def getCQLKeyspace(mode='astra_db'):\n",
        "    if mode == 'astra_db':\n",
        "        return ASTRA_DB_KEYSPACE\n",
        "    else:\n",
        "        raise ValueError('Unsupported CQL Session mode')\n",
        "\n",
        "def getTableCount():\n",
        "  # create a query that counts the number of records of the Astra DB table\n",
        "  query = SimpleStatement(f\"\"\"SELECT COUNT(*) FROM {keyspace}.{table_name};\"\"\")\n",
        "\n",
        "  # execute the query\n",
        "  results = session.execute(query)\n",
        "  return results.one().count\n"
      ],
      "id": "TUDw-07Iq4vE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXCQ6T_Gjk0Oz"
      },
      "source": [
        "## **LLM Provider**\n",
        "\n",
        "In the cell below you can choose between **GCP VertexAI** or **OpenAI** for your LLM services.\n",
        "(See [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for more details).\n",
        "\n",
        "Make sure you set the `llmProvider` variable and supply the corresponding access secrets in the following cell."
      ],
      "id": "QXCQ6T_Gjk0Oz"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pGpzZc5zq4vG"
      },
      "outputs": [],
      "source": [
        "# Set your secret(s) for LLM access:\n",
        "llmProvider = 'OpenAI'  # 'GCP_VertexAI'\n"
      ],
      "id": "pGpzZc5zq4vG"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zOFStlEAq4vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4aa2136-9df0-45ff-c61f-da97e069aea3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your secret for LLM provider \"OpenAI\": ··········\n"
          ]
        }
      ],
      "source": [
        "if llmProvider == 'OpenAI':\n",
        "    apiSecret = getpass(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
        "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
        "elif llmProvider == 'GCP_VertexAI':\n",
        "    # we need a json file\n",
        "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
        "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'No file uploaded. Please re-run the cell.'\n",
        "        )\n",
        "else:\n",
        "    raise ValueError('Unknown/unsupported LLM Provider')"
      ],
      "id": "zOFStlEAq4vH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 1 - Prompts**\n",
        "Prompts give an LLM context to craft an appropriate response.  It can consist of input from the user, additional data that may have been retrived from an external data source, and guidance from the system."
      ],
      "metadata": {
        "id": "mqMXHwj3N82I"
      },
      "id": "mqMXHwj3N82I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts can also be parameterized with a template.  This allows us to build the prompt from user input as well as other structured data.  In the example below, we build 2 prompts from the same template, one for a helpufl chatbot named Jane, and one for a sarcastic chatbot named Bob.  The user asks both how they are doing and what is their name.  We also provide the prompt with an appropriate answer for how the agent is doign based on the personality."
      ],
      "metadata": {
        "id": "jGhpozTpPVK5"
      },
      "id": "jGhpozTpPVK5"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a {personality} AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"{first_response}\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "messages_sarcastic = template.format_messages(\n",
        "    personality='sarcastic',\n",
        "    name=\"Bob\",\n",
        "    first_response=\"Too many queries from dumb people\",\n",
        "    user_input=\"What is your name?\"\n",
        ")\n",
        "\n",
        "messages_helpful = template.format_messages(\n",
        "    personality='helpful',\n",
        "    name=\"Jane\",\n",
        "    first_response=\"I'm doing well, thanks!\",\n",
        "    user_input=\"What is your name?\"\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "sarcastic = llm(messages_sarcastic)\n",
        "print(\"Sarcastic Message: \" , sarcastic)\n",
        "helpful = llm(messages_helpful)\n",
        "print (\"Helfpul Message: \" ,helpful)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0VVcp6GQnYG",
        "outputId": "45d7eb4a-ab2c-4d8b-c465-3854f4d68b77"
      },
      "id": "J0VVcp6GQnYG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarcastic Message:  content=\"Oh, I see you're one of those who ask obvious questions. My name is Bob. Now, what can I do for you?\"\n",
            "Helfpul Message:  content='My name is Jane. How can I assist you today?'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 2 - Chains**\n",
        "\n",
        "Chains allow you to string together a sequence of calls.  We will continue with our template example from above.  We had separate statements setting the inputs and calling the LLM.  With a chain, both can be handled in a single call.  We will construct our prompts slightly different and then send along to the chain."
      ],
      "metadata": {
        "id": "PLrynAB2R2Fp"
      },
      "id": "PLrynAB2R2Fp"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "from langchain.prompts import SystemMessagePromptTemplate\n",
        "from langchain.prompts import AIMessagePromptTemplate\n",
        "\n",
        "human_message_prompt2 = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=\"{user_input}\",\n",
        "            input_variables=[\"user_input\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=\"You are a {personality} AI bot. Your name is {name}.\",\n",
        "            input_variables=[\"personality\", \"name\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "ai_message_prompt = AIMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=\"{first_response}\",\n",
        "            input_variables=[\"first_response\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "human_message_prompt1 = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=\"Hello how are you doing? {user_input2}\",\n",
        "            input_variables=[\"user_input2\"]\n",
        "        )\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "QHm3Sbk9OETf"
      },
      "id": "QHm3Sbk9OETf",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt1, ai_message_prompt, human_message_prompt2])\n",
        "chain = LLMChain(llm=llm, prompt=chat_prompt_template)\n",
        "print(chain.run({\n",
        "    'personality' :\"sarcastic\",\n",
        "    'name' : \"Bob\",\n",
        "    'first_response' : \"Too many queries from dumb people\",\n",
        "    'user_input' : \"What is your name?\",\n",
        "    'user_input2' : \" \"\n",
        "\n",
        "    }))\n",
        "\n",
        "\n",
        "print(chain.run({\n",
        "    'personality' :\"helpful\",\n",
        "    'name' : \"Jane\",\n",
        "    'first_response' : \"I'm doing well, thanks!\",\n",
        "    'user_input' : \"What is your name?\",\n",
        "    'user_input2' : \" \"\n",
        "\n",
        "    }))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uuuXUvnVSop",
        "outputId": "e42247f3-0f34-49b8-e615-066f43d886f1"
      },
      "id": "_uuuXUvnVSop",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, sorry, I forgot to introduce myself. I'm Bob, the sarcastic AI bot. How can I assist you today?\n",
            "My name is Jane. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NGQKOhPRb-M"
      },
      "id": "9NGQKOhPRb-M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 3 - Retrieval (Astra as a Vector DB)**\n",
        "We are going to use the Langchain retrival module by using AstraDB as a vector store.  (This example was meant to build upon the prompt and chain learnings above but I ran out of time)\n",
        "### **Dataset Setup**\n",
        "We will use the US Constitution as our dataset"
      ],
      "metadata": {
        "id": "6Xo9boKEeSwf"
      },
      "id": "6Xo9boKEeSwf"
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://www.govinfo.gov/content/pkg/CDOC-110hdoc50/html/CDOC-110hdoc50.htm > constitution.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O20WtRdvecvx",
        "outputId": "cf377325-cffd-4db4-be3f-4e6d734cd930"
      },
      "id": "O20WtRdvecvx",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  291k    0  291k    0     0   485k      0 --:--:-- --:--:-- --:--:--  486k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "\n",
        "loader = TextLoader(\"constitution.txt\")\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrS7xcpUelzB",
        "outputId": "421186f7-05e7-4066-dc52-a7963aa07e22"
      },
      "id": "NrS7xcpUelzB",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 4562, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 21641, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 6612, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2609, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2239, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1870, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2679, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1111, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1860, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2927, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2233, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2149, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1702, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1615, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1016, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1887, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2394, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2017, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1824, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1588, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1672, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1678, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1721, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2739, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1978, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1591, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1477, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1034, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1025, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 12487, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3409, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 28779, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 9344, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 8742, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2733, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1842, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4688, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 9278, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 7108, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 5295, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 4497, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 3035, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 5293, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 23409, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2916, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 13488, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 19609, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 9183, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1606, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 7562, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2578, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6715bc2b",
      "metadata": {
        "id": "6715bc2b"
      },
      "source": [
        "### **Load into Astra DB**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761d9b70",
      "metadata": {
        "id": "761d9b70"
      },
      "source": [
        "_**NOTE:** this uses Cassandra's \"Vector Similarity Search\" capability.\n",
        "Make sure you are connecting to a vector-enabled database for this demo._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "11013224",
      "metadata": {
        "id": "11013224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed3c1c3-48fb-4e65-b494-3d7595269646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 8dce574d-98e5-4b4d-bdfe-2477340a5d09-us-east1.db.astra.datastax.com:29042:fc9aceac-af5d-4952-b0ee-0c341ee23681. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 8dce574d-98e5-4b4d-bdfe-2477340a5d09-us-east1.db.astra.datastax.com:29042:fc9aceac-af5d-4952-b0ee-0c341ee23681. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(136517019521648) 8dce574d-98e5-4b4d-bdfe-2477340a5d09-us-east1.db.astra.datastax.com:29042:fc9aceac-af5d-4952-b0ee-0c341ee23681> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 8dce574d-98e5-4b4d-bdfe-2477340a5d09-us-east1.db.astra.datastax.com:29042:fc9aceac-af5d-4952-b0ee-0c341ee23681. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ],
      "source": [
        "from cassandra.query import SimpleStatement\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "\n",
        "# creation of the DB connection\n",
        "cqlMode = 'astra_db'\n",
        "session = getCQLSession(mode=cqlMode)\n",
        "keyspace = getCQLKeyspace(mode=cqlMode)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_function = OpenAIEmbeddings()\n",
        "\n",
        "# use LangChain's Cassandra integration to load the chunked documents into Astra\n",
        "docsearch = Cassandra.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embeddings_function,\n",
        "    session=session,\n",
        "    keyspace=keyspace,\n",
        "    table_name='langchain_constitution',\n",
        ")"
      ],
      "metadata": {
        "id": "hjtRBdrUjI-w"
      },
      "id": "hjtRBdrUjI-w",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Retrieve / Query Data from Astra via Vector Search**"
      ],
      "metadata": {
        "id": "hnPiBh-Zoo8b"
      },
      "id": "hnPiBh-Zoo8b"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the role of the Vice President?\"\n",
        "docs = docsearch.similarity_search(query, k=2)"
      ],
      "metadata": {
        "id": "2dvcHqT4jiqr"
      },
      "id": "2dvcHqT4jiqr",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtiDRA13jwMc",
        "outputId": "08d6d8b2-6a90-43ad-90a7-4ac413a08527"
      },
      "id": "UtiDRA13jwMc",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Electors shall meet in their respective states, and \n",
            "vote by ballot for President and Vice-President, one of whom, \n",
            "at least, shall not be an inhabitant of the same state with \n",
            "themselves; they shall name in their ballots the person voted \n",
            "for as President, and in distinct ballots the person voted for \n",
            "as Vice-President, and they shall make distinct lists of all \n",
            "persons voted for as President, and of all persons voted for as \n",
            "Vice-President, and of the number of votes for each, which \n",
            "lists they shall sign and certify, and transmit sealed to the \n",
            "seat of the government of the United States, directed to the \n",
            "President of the Senate;--The President of the Senate shall, in \n",
            "the presence of the Senate and House of Representatives, open \n",
            "all the certificates and the votes shall then be counted;--The \n",
            "person having the greatest number of votes for President, shall \n",
            "be the President, if such number be a majority of the whole \n",
            "number of Electors appointed; and if no person have such \n",
            "majority, then from the persons having the highest numbers not \n",
            "exceeding three on the list of those voted for as President, \n",
            "the House of Representatives shall choose immediately, by \n",
            "ballot, the President. But in choosing the President, the votes \n",
            "shall be taken by states, the representation from each state \n",
            "having one vote; a quorum for this purpose shall consist of a \n",
            "member or members from two-thirds of the states, and a majority \n",
            "of all the states shall be necessary to a choice. And if the \n",
            "House of Representatives shall not choose a President whenever \n",
            "the right of choice shall devolve upon them, before the fourth \n",
            "day of March next following, then the Vice-President shall act \n",
            "as President, as in the case of the death or other \n",
            "constitutional disability of the President.\\14\\--The person \n",
            "having the greatest number of votes as Vice-President, shall be \n",
            "the Vice-President, if such number be a majority of the whole \n",
            "number of Electors appointed, and if no person have a majority, \n",
            "then from the two highest numbers on the list, the Senate shall \n",
            "choose the Vice-President; a quorum for the purpose shall \n",
            "consist of two-thirds of the whole number of Senators, and a \n",
            "majority of the whole number shall be necessary to a choice. \n",
            "But no person constitutionally ineligible to the office of \n",
            "President shall be eligible to that of Vice-President of the \n",
            "United States.\n",
            "---------------------------------------------------------------------------\n",
            "                                   * * * * *                              \n",
            "\\14\\This sentence has been superseded by section 3 of amendment XX.\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}